### Create container for test Merror
CV.Merror = array(0, dim = c(K, n.pars))
for(i in 1:K){
### Print progress update
print(paste0(i, " of ", K))
### Split data and rescale predictors
data.train = set1[folds != i,]
XY.train = data.train
data.valid = set1[folds == i,]
XY.valid =data.valid
### Fit RF models for each parameter combination. A second
### for loop will make our life easier here
for(j in 1:n.pars){
### Get current parameter values
this.mtry = all.pars[j,1]
this.nodesize = all.pars[j,2]
### We need to run RF multiple times to avoid bad local minima. Create
### containers to store the models and their errors.
all.rfs = list(1:M)
all.Merror = rep(0, times = M)
### We need to fit each model multiple times. This calls for another
### for loop.
for(l in 1:M){
### Fit model
wh.rfm <- randomForest(data=XY.train, Accident.Severity~.,
mtry=this.mtry, nodesize=this.nodesize, ntree=1000)
# get misclassification rate
Merror.rf = mean(predict(wh.rfm, type="response") != XY.train$Accident.Severity)
all.rfs[[l]] = wh.rfm
all.Merror[l] = Merror.rf
}
### Get best fit using current parameter values
ind.best = which.min(all.Merror)
fit.rf.best = all.rfs[[ind.best]]
### Get predictions and Test Merror, then store Test Merror
Merror.val.rf = mean(predict(fit.rf.best, type="response", newdata=XY.valid) != XY.valid$Accident.Severity)
CV.Merror[i, j] = 1-Merror.val.rf # Be careful with indices for CV.Merror
}
}
CV.Merror
CV.Merror
dim(CV.Merror)
(mean.MerrorCV = apply(X=CV.Merror, MARGIN=2, FUN=mean))
V=K
(mean.MerrorCV.sd = apply(X=CV.Merror, MARGIN=2, FUN=sd))
Merror.cv.CIl = mean.MerrorCV - qt(p=.975, df=V-1)*mean.MerrorCV.sd/V
Merror.cv.CIu = mean.MerrorCV + qt(p=.975, df=V-1)*mean.MerrorCV.sd/V
round(cbind(Merror.cv.CIl, Merror.cv.CIu),2)
table = round(cbind(Merror.cv.CIl, Merror.cv.CIu,mean.MerrorCV),3)
colnames(table)[3] = "mean"
names.pars = paste0(all.pars$mtries,",",
all.pars$nodesizes)
names.pars
rownames(table)= names.pars
table
colnames(CV.Merror) = names.pars
### Make boxplot
boxplot(CV.Merror, las = 2, main = "Test Accuracy Boxplot")
boxplot(CV.Merror, las = 2, main = "Focused Test Accuracy Boxplot",ylim=c(0.78,0.85))
round(cbind(Merror.cv.CIl, Merror.cv.CIu),3)
########################################
source("Helper Functions.R")
n = nrow(set1)
M = 5 # Number of times to re-fit each model
### Define parameter values and use expand.grid() to get all combinations
all.mtry = c(2,3,4,6,10,18) #mtry
all.nodesize = c(1,3,5,7,10)  # nodesize
all.pars = expand.grid(mtries = all.mtry,
nodesizes = all.nodesize)
View(df)
########################################
source("Helper Functions.R")
n = nrow(set1)
M = 5 # Number of times to re-fit each model
### Define parameter values and use expand.grid() to get all combinations
all.mtry = c(2,3,4,6,10,18) #mtry
all.nodesize = c(1,3,5,7,10)  # nodesize
all.pars = expand.grid(mtries = all.mtry,
nodesizes = all.nodesize)
n.pars = nrow(all.pars) # Number of parameter combinations
K = 10 # Number of folds
### Create folds
folds = get.folds(n, K)
### Create container for test Merror
CV.Merror = array(0, dim = c(K, n.pars))
for(i in 1:K){
### Print progress update
print(paste0(i, " of ", K))
### Split data and rescale predictors
data.train = set1[folds != i,]
XY.train = data.train
data.valid = set1[folds == i,]
XY.valid =data.valid
### Fit RF models for each parameter combination. A second
### for loop will make our life easier here
for(j in 1:n.pars){
### Get current parameter values
this.mtry = all.pars[j,1]
this.nodesize = all.pars[j,2]
### We need to run RF multiple times to avoid bad local minima. Create
### containers to store the models and their errors.
all.rfs = list(1:M)
all.Merror = rep(0, times = M)
### We need to fit each model multiple times. This calls for another
### for loop.
for(l in 1:M){
### Fit model
wh.rfm <- randomForest(data=XY.train, Accident.Severity~.,
mtry=this.mtry, nodesize=this.nodesize, ntree=1000)
# get misclassification rate
Merror.rf = mean(predict(wh.rfm, type="response") != XY.train$Accident.Severity)
all.rfs[[l]] = wh.rfm
all.Merror[l] = Merror.rf
}
### Get best fit using current parameter values
ind.best = which.min(all.Merror)
fit.rf.best = all.rfs[[ind.best]]
### Get predictions and Test Merror, then store Test Merror
Merror.val.rf = mean(predict(fit.rf.best, type="response", newdata=XY.valid) != XY.valid$Accident.Severity)
CV.Merror[i, j] = 1-Merror.val.rf # Be careful with indices for CV.Merror
}
}
CV.Merror
dim(CV.Merror)
(mean.MerrorCV = apply(X=CV.Merror, MARGIN=2, FUN=mean))
V=K
CV.Merror
########################################
source("Helper Functions.R")
n = nrow(set1)
M = 5 # Number of times to re-fit each model
### Define parameter values and use expand.grid() to get all combinations
all.mtry = c(2,3,4,6,10,18) #mtry
all.nodesize = c(1,3,5,7,10)  # nodesize
all.pars = expand.grid(mtries = all.mtry,
nodesizes = all.nodesize)
n.pars = nrow(all.pars) # Number of parameter combinations
K = 10 # Number of folds
### Create folds
folds = get.folds(n, K)
### Create container for test Merror
CV.Merror = array(0, dim = c(K, n.pars))
for(i in 1:K){
### Print progress update
print(paste0(i, " of ", K))
### Split data and rescale predictors
data.train = set1[folds != i,]
XY.train = data.train
data.valid = set1[folds == i,]
XY.valid =data.valid
### Fit RF models for each parameter combination. A second
### for loop will make our life easier here
for(j in 1:n.pars){
### Get current parameter values
this.mtry = all.pars[j,1]
this.nodesize = all.pars[j,2]
### We need to run RF multiple times to avoid bad local minima. Create
### containers to store the models and their errors.
all.rfs = list(1:M)
all.Merror = rep(0, times = M)
### We need to fit each model multiple times. This calls for another
### for loop.
for(l in 1:M){
### Fit model
wh.rfm <- randomForest(data=XY.train, Accident.Severity~.,
mtry=this.mtry, nodesize=this.nodesize, ntree=1000)
# get misclassification rate
Merror.rf = mean(predict(wh.rfm, type="response") != XY.train$Accident.Severity)
all.rfs[[l]] = wh.rfm
all.Merror[l] = Merror.rf
}
### Get best fit using current parameter values
ind.best = which.min(all.Merror)
fit.rf.best = all.rfs[[ind.best]]
### Get predictions and Test Merror, then store Test Merror
Merror.val.rf = mean(predict(fit.rf.best, type="response", newdata=XY.valid) != XY.valid$Accident.Severity)
CV.Merror[i, j] = 1-Merror.val.rf # Be careful with indices for CV.Merror
}
}
CV.Merror
dim(CV.Merror)
(mean.MerrorCV = apply(X=CV.Merror, MARGIN=2, FUN=mean))
V=K
(mean.MerrorCV.sd = apply(X=CV.Merror, MARGIN=2, FUN=sd))
Merror.cv.CIl = mean.MerrorCV - qt(p=.975, df=V-1)*mean.MerrorCV.sd/V
Merror.cv.CIu = mean.MerrorCV + qt(p=.975, df=V-1)*mean.MerrorCV.sd/V
round(cbind(Merror.cv.CIl, Merror.cv.CIu),3)
table = round(cbind(Merror.cv.CIl, Merror.cv.CIu,mean.MerrorCV),3)
colnames(table)[3] = "mean"
names.pars = paste0(all.pars$mtries,",",
all.pars$nodesizes)
names.pars
rownames(table)= names.pars
table
colnames(CV.Merror) = names.pars
### Make boxplot
boxplot(CV.Merror, las = 2, main = "Test Accuracy Boxplot")
CV.Merror
CV.Merror[i, j]
Merror.val.rf
1-Merror.val.rf
n = nrow(set1)
M = 5 # Number of times to re-fit each model
### Define parameter values and use expand.grid() to get all combinations
all.mtry = c(2,3,4,6,10,18) #mtry
all.nodesize = c(1,3,5,7,10)  # nodesize
########################################
source("Helper Functions.R")
n = nrow(set1)
M = 5 # Number of times to re-fit each model
### Define parameter values and use expand.grid() to get all combinations
all.mtry = c(2,3)#,4,6,10,18) #mtry
all.nodesize = c(1)#,3,5,7,10)  # nodesize
all.pars = expand.grid(mtries = all.mtry,
nodesizes = all.nodesize)
n.pars = nrow(all.pars) # Number of parameter combinations
K = 5 # Number of folds
### Create folds
folds = get.folds(n, K)
### Create container for test Merror
CV.Merror = array(0, dim = c(K, n.pars))
for(i in 1:K){
### Print progress update
print(paste0(i, " of ", K))
### Split data and rescale predictors
data.train = set1[folds != i,]
XY.train = data.train
data.valid = set1[folds == i,]
XY.valid =data.valid
### Fit RF models for each parameter combination. A second
### for loop will make our life easier here
for(j in 1:n.pars){
### Get current parameter values
this.mtry = all.pars[j,1]
this.nodesize = all.pars[j,2]
### We need to run RF multiple times to avoid bad local minima. Create
### containers to store the models and their errors.
all.rfs = list(1:M)
all.Merror = rep(0, times = M)
### We need to fit each model multiple times. This calls for another
### for loop.
for(l in 1:M){
### Fit model
wh.rfm <- randomForest(data=XY.train, Accident.Severity~.,
mtry=this.mtry, nodesize=this.nodesize, ntree=1000)
# get misclassification rate
Merror.rf = mean(predict(wh.rfm, type="response") != XY.train$Accident.Severity)
all.rfs[[l]] = wh.rfm
all.Merror[l] = Merror.rf
}
### Get best fit using current parameter values
ind.best = which.min(all.Merror)
fit.rf.best = all.rfs[[ind.best]]
### Get predictions and Test Merror, then store Test Merror
Merror.val.rf = mean(predict(fit.rf.best, type="response", newdata=XY.valid) != XY.valid$Accident.Severity)
CV.Merror[i, j] = 1-Merror.val.rf # Be careful with indices for CV.Merror
}
}
CV.Merror
########################################
source("Helper Functions.R")
n = nrow(set1)
M = 5 # Number of times to re-fit each model
### Define parameter values and use expand.grid() to get all combinations
all.mtry = c(2,3,4,6,10,18) #mtry
all.nodesize = c(1,3,5,7,10)  # nodesize
all.pars = expand.grid(mtries = all.mtry,
nodesizes = all.nodesize)
n.pars = nrow(all.pars) # Number of parameter combinations
K = 5 # Number of folds
### Create folds
folds = get.folds(n, K)
### Create container for test Merror
CV.Merror = array(0, dim = c(K, n.pars))
for(i in 1:K){
### Print progress update
print(paste0(i, " of ", K))
### Split data and rescale predictors
data.train = set1[folds != i,]
XY.train = data.train
data.valid = set1[folds == i,]
XY.valid =data.valid
### Fit RF models for each parameter combination. A second
### for loop will make our life easier here
for(j in 1:n.pars){
### Get current parameter values
this.mtry = all.pars[j,1]
this.nodesize = all.pars[j,2]
### We need to run RF multiple times to avoid bad local minima. Create
### containers to store the models and their errors.
all.rfs = list(1:M)
all.Merror = rep(0, times = M)
### We need to fit each model multiple times. This calls for another
### for loop.
for(l in 1:M){
### Fit model
wh.rfm <- randomForest(data=XY.train, Accident.Severity~.,
mtry=this.mtry, nodesize=this.nodesize, ntree=1000)
# get misclassification rate
Merror.rf = mean(predict(wh.rfm, type="response") != XY.train$Accident.Severity)
all.rfs[[l]] = wh.rfm
all.Merror[l] = Merror.rf
}
### Get best fit using current parameter values
ind.best = which.min(all.Merror)
fit.rf.best = all.rfs[[ind.best]]
### Get predictions and Test Merror, then store Test Merror
Merror.val.rf = mean(predict(fit.rf.best, type="response", newdata=XY.valid) != XY.valid$Accident.Severity)
CV.Merror[i, j] = 1-Merror.val.rf # Be careful with indices for CV.Merror
}
}
CV.Merror
dim(CV.Merror)
(mean.MerrorCV = apply(X=CV.Merror, MARGIN=2, FUN=mean))
V=K
(mean.MerrorCV.sd = apply(X=CV.Merror, MARGIN=2, FUN=sd))
Merror.cv.CIl = mean.MerrorCV - qt(p=.975, df=V-1)*mean.MerrorCV.sd/V
Merror.cv.CIu = mean.MerrorCV + qt(p=.975, df=V-1)*mean.MerrorCV.sd/V
round(cbind(Merror.cv.CIl, Merror.cv.CIu),3)
table = round(cbind(Merror.cv.CIl, Merror.cv.CIu,mean.MerrorCV),3)
colnames(table)[3] = "mean"
names.pars = paste0(all.pars$mtries,",",
all.pars$nodesizes)
names.pars
rownames(table)= names.pars
table
colnames(CV.Merror) = names.pars
### Make boxplot
boxplot(CV.Merror, las = 2, main = "Test Accuracy Boxplot")
CV.Merror
########################################
source("Helper Functions.R")
n = nrow(set1)
M = 5 # Number of times to re-fit each model
### Define parameter values and use expand.grid() to get all combinations
all.mtry = c(2,4,6,10,18) #mtry
all.nodesize = c(1,3,5,7,10)  # nodesize
all.pars = expand.grid(mtries = all.mtry,
nodesizes = all.nodesize)
n.pars = nrow(all.pars) # Number of parameter combinations
K = 5 # Number of folds
### Create folds
folds = get.folds(n, K)
### Create container for test Merror
CV.Merror = array(0, dim = c(K, n.pars))
for(i in 1:K){
### Print progress update
print(paste0(i, " of ", K))
### Split data and rescale predictors
data.train = set1[folds != i,]
XY.train = data.train
data.valid = set1[folds == i,]
XY.valid =data.valid
### Fit RF models for each parameter combination. A second
### for loop will make our life easier here
for(j in 1:n.pars){
### Get current parameter values
this.mtry = all.pars[j,1]
this.nodesize = all.pars[j,2]
### We need to run RF multiple times to avoid bad local minima. Create
### containers to store the models and their errors.
all.rfs = list(1:M)
all.Merror = rep(0, times = M)
### We need to fit each model multiple times. This calls for another
### for loop.
for(l in 1:M){
### Fit model
wh.rfm <- randomForest(data=XY.train, Accident.Severity~.,
mtry=this.mtry, nodesize=this.nodesize, ntree=1000)
# get misclassification rate
Merror.rf = mean(predict(wh.rfm, type="response") != XY.train$Accident.Severity)
all.rfs[[l]] = wh.rfm
all.Merror[l] = Merror.rf
}
### Get best fit using current parameter values
ind.best = which.min(all.Merror)
fit.rf.best = all.rfs[[ind.best]]
### Get predictions and Test Merror, then store Test Merror
Merror.val.rf = mean(predict(fit.rf.best, type="response", newdata=XY.valid) != XY.valid$Accident.Severity)
CV.Merror[i, j] = 1-Merror.val.rf # Be careful with indices for CV.Merror
}
}
CV.Merror
dim(CV.Merror)
(mean.MerrorCV = apply(X=CV.Merror, MARGIN=2, FUN=mean))
(mean.MerrorCV.sd = apply(X=CV.Merror, MARGIN=2, FUN=sd))
Merror.cv.CIl = mean.MerrorCV - qt(p=.975, df=V-1)*mean.MerrorCV.sd/V
Merror.cv.CIu = mean.MerrorCV + qt(p=.975, df=V-1)*mean.MerrorCV.sd/V
round(cbind(Merror.cv.CIl, Merror.cv.CIu),3)
table = round(cbind(Merror.cv.CIl, Merror.cv.CIu,mean.MerrorCV),3)
colnames(table)[3] = "mean"
names.pars = paste0(all.pars$mtries,",",
all.pars$nodesizes)
names.pars
rownames(table)= names.pars
table
colnames(CV.Merror) = names.pars
### Make boxplot
boxplot(CV.Merror, las = 2, main = "Test Accuracy Boxplot")
min.err = apply(CV.Merror, 2, min)
min.err
boxplot(t(CV.Merror)/min.err, use.cols=TRUE, las=2,
main="RF Tuning Variables and Node Sizes")
### Make boxplot
boxplot(CV.Merror, las = 2, main = "RF Tuning Variables and Node Sizes")
CV.Merror
boxplot((CV.Merror)/min.err, use.cols=TRUE, las=2,
main="RF Tuning Variables and Node Sizes")
min.err = apply(CV.Merror, 2, min)
boxplot((CV.Merror)/min.err, use.cols=TRUE, las=2,
main="RF Tuning Variables and Node Sizes")
CV.Merror
min.err
min.err = apply(CV.Merror, 1, min)
boxplot((CV.Merror)/min.err, use.cols=TRUE, las=2,
main="RF Tuning Variables and Node Sizes")
boxplot((CV.Merror)/min.err, use.cols=TRUE, las=2,
main="RF Tuning Variables and Node Sizes (Relative)")
min.err = apply(CV.Merror, 2, min)
boxplot((CV.Merror)/min.err, use.cols=TRUE, las=2,
main="RF Tuning Variables and Node Sizes (Relative)")
min.err = apply(CV.Merror, 1, min)
boxplot((CV.Merror)/min.err, use.cols=TRUE, las=2,
main="RF Tuning Variables and Node Sizes (Relative)")
casualties_test <- read.csv("test.csv", header=T)
head(casualties_test,2)
table(casualties_test$Accident.Severity)
prop.table(table(casualties_test$Accident.Severity))
### swap columns
col_swaps = c('AREFNO','ageband','Accident.Date','Time',
'No..of.Vehicles.in.Acc.','No..of.Casualties',
'Casualty.Age','Casualty.Sex','Casualty.Class',
'Light.Conditions.Banded','Road.Surface','Weather','Junction.Detail',
'Speed.Limit','Road.Type','Highway',
'hour','day','month','Mode.of.Travel',
'Accident.Severity')
casualties_test<-subset(casualties_test, select=col_swaps)
##### data preprocessing #############
cols = c('Casualty.Sex','Casualty.Class',
'Light.Conditions.Banded','Road.Surface','Weather','Junction.Detail',
'Road.Type','Highway',
'day','month','Mode.of.Travel',
'Accident.Severity')
casualties_test[cols] <- lapply(casualties_test[cols], factor)  ## as.factor() could also be used
### make month and day numerical
casualties_test$month=match(casualties_test$month,month.name)
table(casualties_test$day)
df_test = casualties_test
col_features = c("Casualty.Age", "Casualty.Sex",
"Casualty.Class","Light.Conditions.Banded","Road.Surface","Weather",
"Junction.Detail","Speed.Limit","Road.Type", "Highway",
"hour","day","month" ,"Mode.of.Travel","Accident.Severity")
df_test = df_test[,col_features]
set1_test = df_test
set1_test = set1_test
set1_test$Accident.Severity <- factor(set1_test$Accident.Severity,
levels= c("Serious", "Slight"),
labels= c(1, 0))
cols = c('Casualty.Sex','Casualty.Class',
'Light.Conditions.Banded','Road.Surface','Weather','Junction.Detail',
'Road.Type','Highway',
'day','month','Mode.of.Travel')
set1_test[cols] <- lapply(set1_test[cols], as.numeric)
set1_mt_test <- xgb.DMatrix(data = as.matrix(set1_test[,-15]), label= as.matrix(set1_test[,15]))
#####################################################
# rf.tun <- randomForest(data=set1, Accident.Severity~., mtry=2, nodesize=5,ntree=1000,
# importance=TRUE, keep.forest=TRUE)
rf.tun <- randomForest(data=df, Accident.Severity~., mtry=4, nodesize=2,ntree=1000,
importance=TRUE, keep.forest=TRUE)
# # Predict results of classification.
pred.rf.test.tun <- predict(rf.tun, newdata=df_test, type="response")
# (misclass.train.rf.tun <- mean(ifelse(pred.rf.train.tun == set1$Y, yes=0, no=1)))
(misclass.test.rf.tun <- mean(ifelse(pred.rf.test.tun != df_test$Accident.Severity, yes=0, no=1)))
#
table(df_test[,'Accident.Severity'],pred.rf.test.tun, dnn=c("Observed","Predicted"))
confusionMatrix(factor(pred.rf.test.tun), factor(df_test[,'Accident.Severity']),mode='everything')
#####################################################
# rf.tun <- randomForest(data=set1, Accident.Severity~., mtry=2, nodesize=5,ntree=1000,
# importance=TRUE, keep.forest=TRUE)
rf.tun <- randomForest(data=df, Accident.Severity~., mtry=4, nodesize=3,ntree=1000,
importance=TRUE, keep.forest=TRUE)
# # Predict results of classification.
pred.rf.test.tun <- predict(rf.tun, newdata=df_test, type="response")
# (misclass.train.rf.tun <- mean(ifelse(pred.rf.train.tun == set1$Y, yes=0, no=1)))
(misclass.test.rf.tun <- mean(ifelse(pred.rf.test.tun != df_test$Accident.Severity, yes=0, no=1)))
#
table(df_test[,'Accident.Severity'],pred.rf.test.tun, dnn=c("Observed","Predicted"))
confusionMatrix(factor(pred.rf.test.tun), factor(df_test[,'Accident.Severity']),mode='everything')
#####################################################
# rf.tun <- randomForest(data=set1, Accident.Severity~., mtry=2, nodesize=5,ntree=1000,
# importance=TRUE, keep.forest=TRUE)
rf.tun <- randomForest(data=df, Accident.Severity~., mtry=2, nodesize=1,ntree=1000,
importance=TRUE, keep.forest=TRUE)
# # Predict results of classification.
pred.rf.test.tun <- predict(rf.tun, newdata=df_test, type="response")
# (misclass.train.rf.tun <- mean(ifelse(pred.rf.train.tun == set1$Y, yes=0, no=1)))
(misclass.test.rf.tun <- mean(ifelse(pred.rf.test.tun != df_test$Accident.Severity, yes=0, no=1)))
#
table(df_test[,'Accident.Severity'],pred.rf.test.tun, dnn=c("Observed","Predicted"))
confusionMatrix(factor(pred.rf.test.tun), factor(df_test[,'Accident.Severity']),mode='everything')
#####################################################
# rf.tun <- randomForest(data=set1, Accident.Severity~., mtry=2, nodesize=5,ntree=1000,
# importance=TRUE, keep.forest=TRUE)
rf.tun <- randomForest(data=df, Accident.Severity~., mtry=2, nodesize=2,ntree=1000,
importance=TRUE, keep.forest=TRUE)
# # Predict results of classification.
pred.rf.test.tun <- predict(rf.tun, newdata=df_test, type="response")
# (misclass.train.rf.tun <- mean(ifelse(pred.rf.train.tun == set1$Y, yes=0, no=1)))
(misclass.test.rf.tun <- mean(ifelse(pred.rf.test.tun != df_test$Accident.Severity, yes=0, no=1)))
#
table(df_test[,'Accident.Severity'],pred.rf.test.tun, dnn=c("Observed","Predicted"))
confusionMatrix(factor(pred.rf.test.tun), factor(df_test[,'Accident.Severity']),mode='everything')
